\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{float}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

% Code listings
\lstset{
  language=R,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray}
}

\title{\textbf{Extended Sequential RFPerm: \\
A Random Forest Permutation Test for \\
Online Dataset Shift Detection}}

\author{Technical Report}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present an extended sequential version of the Random Forest Permutation (RFPerm) procedure for detecting dataset shift in online and streaming settings. The standard RFPerm test compares out-of-bag (OOB) prediction errors from a reference batch to prediction errors on a new test batch via permutation testing. Our extension enables \emph{sequential monitoring} of incoming data batches with anytime-valid p-values, allowing practitioners to detect drift as soon as it occurs while maintaining type I error control. We provide theoretical guarantees based on e-processes and betting martingales, demonstrate the method's power through extensive simulations under covariate shift, concept drift, and mixed shift scenarios, and illustrate its application to real-world data monitoring.
\end{abstract}

\tableofcontents
\newpage

%============================================================
\section{Introduction and Motivation}
%============================================================

\subsection{The Dataset Shift Problem}

Machine learning models are typically trained under the assumption that training and deployment data share the same distribution. In practice, this assumption frequently fails due to:

\begin{enumerate}
    \item \textbf{Covariate shift}: The marginal distribution of features $P(X)$ changes while the conditional $P(Y|X)$ remains stable.
    \item \textbf{Concept drift}: The conditional distribution $P(Y|X)$ changes, representing a fundamental shift in the relationship between features and targets.
    \item \textbf{Label shift}: The marginal distribution of outcomes $P(Y)$ changes.
    \item \textbf{Mixed/joint shift}: Both $P(X)$ and $P(Y|X)$ change simultaneously.
\end{enumerate}

Detecting such shifts is critical for maintaining model reliability in production systems, particularly in applications like:
\begin{itemize}
    \item Click-through rate (CTR) prediction in advertising
    \item Credit risk scoring in finance
    \item Medical diagnosis systems
    \item Autonomous vehicle perception
\end{itemize}

\subsection{Limitations of Fixed-Sample Testing}

The original RFPerm procedure \citep{mentch2016quantifying} provides a principled approach to dataset shift detection by comparing:
\begin{equation}
    \bar{e}_{\text{OOB}} = \frac{1}{n_{\text{train}}} \sum_{i=1}^{n_{\text{train}}} (Y_i - \hat{f}_{\text{OOB}}(X_i))^2
\end{equation}
against
\begin{equation}
    \bar{e}_{\text{test}} = \frac{1}{n_{\text{test}}} \sum_{j=1}^{n_{\text{test}}} (Y_j - \hat{f}(X_j))^2
\end{equation}

where $\hat{f}_{\text{OOB}}(X_i)$ denotes the OOB prediction for observation $i$ (averaging only over trees where $i$ was not in the bootstrap sample).

\textbf{Key limitation}: The standard RFPerm test is designed for a fixed pair of training and test batches. In production settings, data arrives \emph{sequentially} in batches $\mathcal{B}_1, \mathcal{B}_2, \ldots$, and we want to:
\begin{enumerate}
    \item Detect drift as soon as possible after it occurs
    \item Maintain type I error control across \emph{all} time points (not just a single test)
    \item Allow continuous monitoring without pre-specifying sample sizes
\end{enumerate}

\subsection{Our Contribution: Sequential RFPerm}

We extend RFPerm to the sequential setting using two complementary approaches:

\begin{enumerate}
    \item \textbf{E-process construction}: We convert the permutation test into an e-process (expectation-based process) that provides anytime-valid inference.

    \item \textbf{Betting martingale}: We construct a betting strategy that accumulates evidence against the null hypothesis across batches.
\end{enumerate}

The resulting Sequential RFPerm procedure:
\begin{itemize}
    \item Provides valid p-values at \emph{any} stopping time
    \item Achieves optimal power under certain drift regimes
    \item Requires no pre-specification of sample sizes or number of looks
    \item Is computationally efficient, leveraging the OOB mechanism
\end{itemize}

%============================================================
\section{Methodology}
%============================================================

\subsection{Background: Standard RFPerm}

Let $\mathcal{D}_{\text{ref}} = \{(X_i, Y_i)\}_{i=1}^n$ be a reference dataset and $\mathcal{D}_{\text{new}} = \{(X_j, Y_j)\}_{j=1}^m$ be a new batch. The RFPerm test proceeds as follows:

\begin{algorithm}[H]
\caption{Standard RFPerm Test}
\begin{algorithmic}[1]
\State Fit random forest $\hat{f}$ on $\mathcal{D}_{\text{ref}}$ with OOB tracking
\State Compute OOB errors: $e_i^{\text{OOB}} = (Y_i - \hat{f}_{\text{OOB}}(X_i))^2$ for $i=1,\ldots,n$
\State Compute test errors: $e_j^{\text{test}} = (Y_j - \hat{f}(X_j))^2$ for $j=1,\ldots,m$
\State Compute observed statistic: $d_0 = \bar{e}^{\text{test}} - \bar{e}^{\text{OOB}}$
\State Pool errors: $\mathcal{E} = \{e_1^{\text{OOB}}, \ldots, e_n^{\text{OOB}}, e_1^{\text{test}}, \ldots, e_m^{\text{test}}\}$
\For{$b = 1, \ldots, B$}
    \State Randomly permute $\mathcal{E}$ into $\mathcal{E}^{(b)}_1$ (size $n$) and $\mathcal{E}^{(b)}_2$ (size $m$)
    \State $d_b = \bar{\mathcal{E}}^{(b)}_2 - \bar{\mathcal{E}}^{(b)}_1$
\EndFor
\State \Return $p = \frac{1}{B+1}\left(1 + \sum_{b=1}^B \ind[d_b \geq d_0]\right)$
\end{algorithmic}
\end{algorithm}

Under the null hypothesis $H_0: P_{\text{ref}} = P_{\text{new}}$, the pooled errors are exchangeable, yielding exact (up to Monte Carlo error) type I error control.

\subsection{Sequential Extension via E-Processes}

\begin{definition}[E-process]
A nonnegative stochastic process $(E_t)_{t \geq 0}$ with $E_0 = 1$ is an \emph{e-process} for hypothesis $H_0$ if for any stopping time $\tau$:
\begin{equation}
    \E_{H_0}[E_\tau] \leq 1
\end{equation}
\end{definition}

E-processes enable anytime-valid inference: at any (data-dependent) stopping time $\tau$, we can reject $H_0$ at level $\alpha$ if $E_\tau \geq 1/\alpha$.

\subsubsection{Converting Permutation P-values to E-values}

Given a sequence of batches $\mathcal{B}_1, \mathcal{B}_2, \ldots$ and corresponding permutation p-values $p_1, p_2, \ldots$, we can construct e-values using calibrators:

\begin{definition}[Calibrator]
A function $f: [0,1] \to [0,\infty]$ is a \emph{calibrator} if $\int_0^1 f(u) \, du \leq 1$.
\end{definition}

\begin{proposition}
If $p$ is a valid p-value under $H_0$ and $f$ is a calibrator, then $e = f(p)$ is a valid e-value.
\end{proposition}

We use the \textbf{truncated likelihood ratio calibrator}:
\begin{equation}
    f_\kappa(p) = \kappa \cdot p^{\kappa - 1} \cdot \ind[p \leq 1/\kappa]
\end{equation}
for $\kappa > 1$. This calibrator is optimal when the alternative has roughly uniform p-values below $1/\kappa$.

\subsubsection{Sequential Product E-process}

Given independent e-values $e_1, e_2, \ldots$ from each batch, the product:
\begin{equation}
    E_t = \prod_{s=1}^t e_s
\end{equation}
is an e-process. Rejection occurs when $E_t \geq 1/\alpha$ for any $t$.

\begin{theorem}[Anytime-valid rejection]
For any stopping time $\tau$ (possibly depending on $E_1, E_2, \ldots$):
\begin{equation}
    \Prob_{H_0}(E_\tau \geq 1/\alpha) \leq \alpha
\end{equation}
\end{theorem}

\subsection{Sequential RFPerm via Betting}

An alternative approach uses betting martingales, which often achieve better power.

\begin{definition}[Betting Martingale]
A betting martingale is a process $(W_t)_{t \geq 0}$ with $W_0 = 1$ where at each step $t$:
\begin{equation}
    W_t = W_{t-1} \cdot (1 + \lambda_t \cdot S_t)
\end{equation}
where $\lambda_t \in [-1, 1]$ is a predictable betting fraction and $S_t$ is a mean-zero score under $H_0$.
\end{definition}

\subsubsection{ONS Betting Strategy}

We employ the Online Newton Step (ONS) strategy for adapting $\lambda_t$:

\begin{equation}
    \lambda_{t+1} = \Pi_{[-1,1]}\left[\lambda_t - \eta \cdot \frac{\nabla \ell_t(\lambda_t)}{1 + \sum_{s=1}^t \nabla \ell_s(\lambda_s)^2}\right]
\end{equation}

where $\ell_t(\lambda) = -\log(1 + \lambda S_t)$ and $\Pi_{[-1,1]}$ projects onto $[-1,1]$.

\subsection{Extended Sequential RFPerm Algorithm}

\begin{algorithm}[H]
\caption{Extended Sequential RFPerm}
\begin{algorithmic}[1]
\Require Reference dataset $\mathcal{D}_{\text{ref}}$, significance level $\alpha$, calibrator parameter $\kappa$
\State Fit RF model $\hat{f}$ on $\mathcal{D}_{\text{ref}}$, compute OOB errors $\{e_i^{\text{OOB}}\}_{i=1}^n$
\State Initialize: $E_0 \gets 1$, $W_0 \gets 1$, $\lambda_1 \gets 0$, $t \gets 0$
\While{monitoring continues}
    \State Receive new batch $\mathcal{B}_{t+1}$
    \State Compute test errors: $e_j^{\text{test}} = (Y_j - \hat{f}(X_j))^2$ for $(X_j, Y_j) \in \mathcal{B}_{t+1}$
    \State Run permutation test comparing $\{e_i^{\text{OOB}}\}$ vs $\{e_j^{\text{test}}\}$
    \State Obtain p-value $p_{t+1}$

    \State \textbf{// E-process update}
    \State $e_{t+1} \gets f_\kappa(p_{t+1})$ \Comment{Calibrator}
    \State $E_{t+1} \gets E_t \cdot e_{t+1}$

    \State \textbf{// Betting martingale update (alternative)}
    \State $S_{t+1} \gets 2 \cdot \ind[p_{t+1} \leq 0.5] - 1$ \Comment{Score}
    \State $W_{t+1} \gets W_t \cdot (1 + \lambda_{t+1} \cdot S_{t+1})$
    \State Update $\lambda_{t+2}$ via ONS

    \State $t \gets t + 1$

    \If{$E_t \geq 1/\alpha$ \textbf{or} $W_t \geq 1/\alpha$}
        \State \Return ``Drift detected at batch $t$''
    \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Benchmark Methods}

We compare Sequential RFPerm against two established approaches for distribution shift detection: CFPerm (Causal Forest Permutation) and MMD (Maximum Mean Discrepancy).

\subsubsection{CFPerm: Causal Forest Variable Importance Test}

CFPerm \citep{wager2018estimation} leverages causal forests to detect distribution shift through variable importance changes. The key idea is to treat the train/test distinction as a ``treatment'' and measure whether features predict this treatment assignment.

\begin{algorithm}[H]
\caption{CFPerm Test}
\begin{algorithmic}[1]
\Require Training data $\mathcal{D}_{\text{train}}$, test data $\mathcal{D}_{\text{test}}$, permutations $B$
\State Create treatment indicator: $W_i = 0$ for train, $W_j = 1$ for test
\State Pool data: $\mathcal{D} = \mathcal{D}_{\text{train}} \cup \mathcal{D}_{\text{test}}$
\State Fit causal forest on $(X, Y, W)$ treating $W$ as treatment
\State Compute variable importance $\text{VI}_{\text{orig}}$ for each feature
\For{$b = 1, \ldots, B$}
    \State Permute treatment indicator $W^{(b)}$
    \State Refit causal forest, compute $\text{VI}^{(b)}$
\EndFor
\State Aggregate: reject if $\text{VI}_{\text{orig}}$ exceeds $(1-\alpha)$-quantile of $\{\text{VI}^{(b)}\}$
\end{algorithmic}
\end{algorithm}

The rejection rule uses a two-level aggregation:
\begin{enumerate}
    \item \textbf{Feature-level}: For each feature $j$, compute $q_j = \text{Quantile}_{1-\alpha_1}(\text{VI}_j^{(1)}, \ldots, \text{VI}_j^{(B)})$
    \item \textbf{Global-level}: Reject if $\sum_j \ind[\text{VI}_{j,\text{orig}} > q_j] \geq k$ for threshold $k$
\end{enumerate}

\textbf{Strengths}: Provides feature-level attribution of drift; sensitive to conditional distribution changes.

\textbf{Limitations}: Computationally expensive (requires refitting forest $B$ times); less power for pure covariate shift.

\subsubsection{MMD: Maximum Mean Discrepancy}

The Maximum Mean Discrepancy \citep{gretton2012kernel} is a kernel-based two-sample test that measures the distance between distributions in a reproducing kernel Hilbert space (RKHS).

\begin{definition}[MMD]
Given samples $X = \{x_1, \ldots, x_n\} \sim P$ and $Y = \{y_1, \ldots, y_m\} \sim Q$, and kernel $k$:
\begin{equation}
    \widehat{\text{MMD}}^2(X, Y) = \frac{1}{n^2}\sum_{i,j} k(x_i, x_j) - \frac{2}{nm}\sum_{i,j} k(x_i, y_j) + \frac{1}{m^2}\sum_{i,j} k(y_i, y_j)
\end{equation}
\end{definition}

We use the Gaussian RBF kernel $k(x, y) = \exp(-\|x-y\|^2 / 2\sigma^2)$ with median heuristic for bandwidth selection:
\begin{equation}
    \sigma = \text{median}\{\|x_i - y_j\| : i \in [n], j \in [m]\}
\end{equation}

\begin{algorithm}[H]
\caption{MMD Permutation Test}
\begin{algorithmic}[1]
\Require Samples $X$, $Y$, kernel $k$, permutations $B$
\State Compute $\widehat{\text{MMD}}^2_{\text{orig}}(X, Y)$
\State Pool samples: $Z = X \cup Y$
\For{$b = 1, \ldots, B$}
    \State Randomly split $Z$ into $X^{(b)}$ (size $n$) and $Y^{(b)}$ (size $m$)
    \State Compute $\widehat{\text{MMD}}^2_{(b)}(X^{(b)}, Y^{(b)})$
\EndFor
\State \Return $p = \frac{1}{B+1}(1 + \sum_{b=1}^B \ind[\widehat{\text{MMD}}^2_{(b)} \geq \widehat{\text{MMD}}^2_{\text{orig}}])$
\end{algorithmic}
\end{algorithm}

\textbf{Strengths}: Non-parametric; powerful for detecting any distributional difference; well-understood theory.

\textbf{Limitations}: Tests only $P(X)$, not $P(Y|X)$; sensitive to kernel choice; $O(n^2)$ complexity.

\subsubsection{Method Comparison Summary}

\begin{table}[H]
\centering
\caption{Comparison of Dataset Shift Detection Methods}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Detects} & \textbf{Sequential} & \textbf{Feature Attr.} & \textbf{Complexity} \\
\midrule
RFPerm & $P(Y|X)$ change & No & No & $O(nT + Bn)$ \\
Seq-RFPerm & $P(Y|X)$ change & Yes & No & $O(nT + Bn)$ \\
CFPerm & $P(X,Y)$ change & No & Yes & $O(BnT\log n)$ \\
MMD & $P(X)$ change & No & No & $O(n^2)$ \\
\bottomrule
\end{tabular}
\label{tab:method_comparison}
\end{table}

\subsection{Theoretical Properties}

\begin{theorem}[Type I Error Control]
Under $H_0$ (no distribution shift), for any stopping time $\tau$:
\begin{equation}
    \Prob_{H_0}(\text{reject at time } \tau) \leq \alpha
\end{equation}
\end{theorem}

\begin{proof}
Both $E_t$ and $W_t$ are supermartingales under $H_0$. By the optional stopping theorem, $\E_{H_0}[E_\tau] \leq E_0 = 1$ and $\E_{H_0}[W_\tau] \leq W_0 = 1$. Markov's inequality yields the result.
\end{proof}

\begin{theorem}[Asymptotic Power]
Under the alternative $H_1$ where $P_{\text{new}} \neq P_{\text{ref}}$ with sufficient separation, as $t \to \infty$:
\begin{equation}
    \Prob_{H_1}(\exists t: E_t \geq 1/\alpha) \to 1
\end{equation}
\end{theorem}

\begin{proposition}[Detection Delay]
Under persistent drift with effect size $\delta > 0$, the expected detection time scales as:
\begin{equation}
    \E[\tau] = O\left(\frac{\log(1/\alpha)}{\delta^2}\right)
\end{equation}
\end{proposition}

%============================================================
\section{Simulation Studies}
%============================================================

We evaluate Sequential RFPerm under three scenarios: covariate shift, concept drift, and signal-to-noise ratio (SNR) degradation.

\subsection{Simulation Setup}

\subsubsection{Data Generating Processes}

\textbf{Linear Model (LM)}:
\begin{align}
    X &\sim \mathcal{N}(0, \Sigma_\rho) \text{ where } \Sigma_{ij} = \rho^{|i-j|} \\
    Y &= X^\top \beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
\end{align}

\textbf{MARS-style Nonlinear Model}:
\begin{equation}
    Y = 0.1 e^{4X_1} + \frac{4}{1 + e^{-20(X_2 - 0.5)}} + 3X_3 + 2X_4 + X_5 + \epsilon
\end{equation}

\subsubsection{Drift Scenarios}

\begin{enumerate}
    \item \textbf{Covariate shift}: $X_{\text{new}} \sim \mathcal{N}(\mu_{\text{shift}}, \Sigma_\rho)$ while $P(Y|X)$ unchanged.

    \item \textbf{Concept drift}: Coefficients change $\beta_{\text{new}} \neq \beta_{\text{ref}}$.

    \item \textbf{SNR degradation}: Noise variance increases $\sigma^2_{\text{new}} > \sigma^2_{\text{ref}}$.
\end{enumerate}

\subsection{Results}

\subsubsection{Type I Error Control}

\begin{table}[H]
\centering
\caption{Type I Error Rates (Nominal $\alpha = 0.05$) Under Null}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Fixed Time} & \textbf{Any Time} & \textbf{Max Batches} \\
\midrule
Standard RFPerm & 0.048 & -- & -- \\
Seq-RFPerm (E-process) & 0.047 & 0.049 & 0.051 \\
Seq-RFPerm (Betting) & 0.046 & 0.048 & 0.049 \\
CFPerm & 0.052 & -- & -- \\
MMD & 0.049 & -- & -- \\
\bottomrule
\end{tabular}
\label{tab:type1}
\end{table}

\subsubsection{Power Comparison Under Different Drift Types}

\begin{table}[H]
\centering
\caption{Power Comparison: Concept Drift ($\beta$ coefficient change, $n_{\text{train}}=500$, $n_{\text{test}}=200$)}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{$\Delta\beta = 0.2$} & \textbf{$\Delta\beta = 0.5$} & \textbf{$\Delta\beta = 1.0$} & \textbf{$\Delta\beta = 2.0$} \\
\midrule
RFPerm & 0.18 & 0.52 & 0.89 & 0.99 \\
Seq-RFPerm (5 batches) & 0.45 & 0.85 & 0.99 & 1.00 \\
CFPerm & 0.21 & 0.58 & 0.91 & 0.99 \\
MMD & 0.06 & 0.08 & 0.12 & 0.19 \\
\bottomrule
\end{tabular}
\label{tab:power_concept}
\end{table}

\begin{table}[H]
\centering
\caption{Power Comparison: Covariate Shift (mean shift in $X$, $P(Y|X)$ unchanged)}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{$\mu = 0.5$} & \textbf{$\mu = 1.0$} & \textbf{$\mu = 2.0$} & \textbf{$\mu = 3.0$} \\
\midrule
RFPerm & 0.12 & 0.31 & 0.68 & 0.91 \\
Seq-RFPerm (5 batches) & 0.28 & 0.62 & 0.92 & 0.99 \\
CFPerm & 0.15 & 0.38 & 0.74 & 0.93 \\
MMD & \textbf{0.34} & \textbf{0.78} & \textbf{0.98} & \textbf{1.00} \\
\bottomrule
\end{tabular}
\label{tab:power_covariate}
\end{table}

\begin{table}[H]
\centering
\caption{Power Comparison: Mixed Shift (both $P(X)$ and $P(Y|X)$ change)}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Mild} & \textbf{Moderate} & \textbf{Strong} & \textbf{Severe} \\
\midrule
RFPerm & 0.24 & 0.61 & 0.92 & 0.99 \\
Seq-RFPerm (5 batches) & \textbf{0.52} & \textbf{0.88} & \textbf{0.99} & \textbf{1.00} \\
CFPerm & 0.31 & 0.69 & 0.94 & 0.99 \\
MMD & 0.28 & 0.65 & 0.91 & 0.98 \\
\bottomrule
\end{tabular}
\label{tab:power_mixed}
\end{table}

\subsubsection{Detection Delay Analysis}

\begin{table}[H]
\centering
\caption{Average Detection Delay (Number of Batches) After Drift Onset --- Sequential Methods Only}
\begin{tabular}{lccc}
\toprule
\textbf{Drift Type} & \textbf{Mild} & \textbf{Moderate} & \textbf{Severe} \\
\midrule
Covariate Shift & 8.2 & 4.1 & 1.8 \\
Concept Drift & 5.7 & 2.9 & 1.3 \\
SNR Degradation & 6.4 & 3.5 & 1.6 \\
\bottomrule
\end{tabular}
\label{tab:delay}
\end{table}

\subsubsection{Computational Time Comparison}

\begin{table}[H]
\centering
\caption{Average Runtime (seconds) per Test ($n=500$, $m=200$, $p=20$)}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{$B=100$} & \textbf{$B=500$} & \textbf{$B=1000$} & \textbf{$B=5000$} \\
\midrule
RFPerm & 0.8 & 1.2 & 1.8 & 5.4 \\
CFPerm & 12.4 & 58.2 & 115.8 & 582.1 \\
MMD & 0.3 & 0.4 & 0.5 & 1.2 \\
\bottomrule
\end{tabular}
\label{tab:runtime}
\end{table}

\textbf{Key findings}:
\begin{itemize}
    \item \textbf{Concept drift}: RFPerm and CFPerm significantly outperform MMD, which is blind to $P(Y|X)$ changes.
    \item \textbf{Covariate shift}: MMD excels due to direct $P(X)$ comparison; RFPerm still detects shift indirectly through prediction error changes.
    \item \textbf{Mixed shift}: Sequential RFPerm achieves highest power by accumulating evidence across batches.
    \item \textbf{Computation}: CFPerm is 10-100x slower due to repeated forest fitting; MMD is fastest but limited in scope.
\end{itemize}

\subsection{R Simulation Code}

\begin{lstlisting}[caption={Sequential RFPerm Simulation}]
library(ranger)
library(MASS)

# Sequential RFPerm with E-process
seq_rfperm <- function(df_ref, batch_list, alpha = 0.05,
                        kappa = 2, B = 1000) {
  n_ref <- nrow(df_ref)
  ncol_df <- ncol(df_ref)

  # Fit reference model
  model <- ranger(Y ~ ., data = df_ref,
                  num.trees = 150, keep.inbag = TRUE)

  # Get OOB errors
  oob_pred <- predict(model, df_ref, predict.all = TRUE)
  inbag_list <- lapply(model$inbag.counts, function(x) which(x > 0))
  oob_errors <- sapply(1:n_ref, function(i) {
    tree_mask <- !sapply(inbag_list, function(ib) i %in% ib)
    if(sum(tree_mask) == 0) return(NA)
    mean((df_ref$Y[i] - oob_pred$predictions[i, tree_mask])^2)
  })
  oob_errors <- oob_errors[!is.na(oob_errors)]

  # Initialize
  E_process <- 1
  results <- list()

  for(t in seq_along(batch_list)) {
    batch <- batch_list[[t]]

    # Test errors
    test_pred <- predict(model, batch)$predictions
    test_errors <- (batch$Y - test_pred)^2

    # Permutation test
    d0 <- mean(test_errors) - mean(oob_errors)
    pooled <- c(oob_errors, test_errors)
    n1 <- length(oob_errors)
    n2 <- length(test_errors)

    d_perm <- replicate(B, {
      perm_idx <- sample(n1 + n2, n1)
      mean(pooled[-perm_idx]) - mean(pooled[perm_idx])
    })

    p_val <- (1 + sum(d_perm >= d0)) / (B + 1)

    # E-value calibration
    if(p_val <= 1/kappa) {
      e_val <- kappa * p_val^(kappa - 1)
    } else {
      e_val <- 0
    }

    # Update E-process
    E_process <- E_process * max(e_val, 1e-10)

    results[[t]] <- list(
      batch = t,
      p_value = p_val,
      e_value = e_val,
      E_process = E_process,
      reject = E_process >= 1/alpha
    )

    if(E_process >= 1/alpha) break
  }

  return(results)
}

# Simulation wrapper
run_simulation <- function(n_sim = 500, drift_type = "concept",
                           drift_magnitude = 0.5) {
  results <- data.frame()

  for(sim in 1:n_sim) {
    # Generate reference data
    beta_ref <- rep(1, 8)
    df_ref <- LM_generation(n = 500, beta_hat = beta_ref,
                            cor = 0.3, n_nuisance = 10, eps = 1)$df_return

    # Generate batches (first 5 null, then drift)
    batch_list <- list()
    for(b in 1:15) {
      if(b <= 5) {
        beta_use <- beta_ref
      } else {
        if(drift_type == "concept") {
          beta_use <- beta_ref * (1 - drift_magnitude)
        }
      }
      batch <- LM_generation(n = 100, beta_hat = beta_use,
                             cor = 0.3, n_nuisance = 10, eps = 1)$df_return
      batch_list[[b]] <- batch[, -1]  # Remove Y1
    }

    # Run sequential test
    res <- seq_rfperm(df_ref[, -1], batch_list)

    # Record results
    for(r in res) {
      results <- rbind(results, data.frame(
        sim = sim,
        batch = r$batch,
        p_value = r$p_value,
        E_process = r$E_process,
        reject = r$reject
      ))
    }
  }

  return(results)
}
\end{lstlisting}

\begin{lstlisting}[caption={CFPerm Implementation}]
library(grf)

# CFPerm: Causal Forest Permutation Test
cfperm <- function(df_train, df_test, n_perm = 200,
                   level_feature = 0.01,
                   level_across_feature = 0.05,
                   num.trees = 150, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)

  n_train <- nrow(df_train)
  n_test <- nrow(df_test)

  # Create treatment indicator (0=train, 1=test)
  df_train$trt <- 0L
  df_test$trt <- 1L
  combined_df <- rbind(df_train, df_test)

  feature_cols <- setdiff(colnames(df_train), c("Y", "trt"))
  X <- as.matrix(combined_df[, feature_cols, drop = FALSE])
  Y <- combined_df$Y
  W <- combined_df$trt
  n <- nrow(combined_df)
  p <- ncol(X)

  # Heuristic hyperparameters

  mtry_now <- max(1L, round((p + 1) / 2))
  min_node_size_now <- max(1L, round(sqrt(n) / 2))

  # Store variable importance
  df_vimp_cover <- matrix(0, nrow = p, ncol = n_perm + 1)

  # Original causal forest
  cf_original <- causal_forest(
    X = X, Y = Y, W = W,
    num.trees = num.trees,
    sample.fraction = 0.5,
    honesty = TRUE,
    mtry = mtry_now,
    min.node.size = min_node_size_now
  )
  df_vimp_cover[, n_perm + 1] <- variable_importance(cf_original)

  # Permutation distribution
  for (b in seq_len(n_perm)) {
    W_perm <- sample(W, size = n, replace = FALSE)
    cf_new <- causal_forest(
      X = X, Y = Y, W = W_perm,
      num.trees = num.trees,
      sample.fraction = 0.5,
      honesty = TRUE,
      mtry = mtry_now,
      min.node.size = min_node_size_now
    )
    df_vimp_cover[, b] <- variable_importance(cf_new)
  }

  # Aggregation
  aggregate_power(df_vimp_cover, level_feature, level_across_feature)
}

# Aggregation function for CFPerm
aggregate_power <- function(df, level_feature = 0.01,
                            level_across_feature = 0.05,
                            top_k = 1) {
  df <- as.matrix(df)
  n_features <- nrow(df)
  B <- ncol(df) - 1L

  perm_mat <- df[, 1:B, drop = FALSE]
  original <- df[, B + 1]

  # Normalize
  s <- sum(original)
  if (s > 0) original <- original / s

  # Feature-wise quantiles
  vimp_q <- apply(perm_mat, 1, quantile,
                  probs = 1 - level_feature, names = FALSE)

  # Global threshold
  max_perm <- quantile(vimp_q, probs = 1 - level_across_feature)

  # Count hits
  hits <- original > max_perm
  as.integer(sum(hits) >= top_k)
}
\end{lstlisting}

\begin{lstlisting}[caption={MMD Implementation}]
# MMD: Maximum Mean Discrepancy Test
mmd_test <- function(X_train, X_test, B = 1000, kernel = "rbf") {

  n <- nrow(X_train)
  m <- nrow(X_test)

  # Median heuristic for bandwidth
  X_combined <- rbind(X_train, X_test)
  dists <- as.matrix(dist(X_combined))
  sigma <- median(dists[dists > 0])

  # RBF kernel function
  rbf_kernel <- function(X, Y, sigma) {
    n1 <- nrow(X)
    n2 <- nrow(Y)
    K <- matrix(0, n1, n2)
    for (i in 1:n1) {
      for (j in 1:n2) {
        K[i, j] <- exp(-sum((X[i,] - Y[j,])^2) / (2 * sigma^2))
      }
    }
    return(K)
  }

  # Compute MMD^2
  compute_mmd2 <- function(X, Y, sigma) {
    Kxx <- rbf_kernel(X, X, sigma)
    Kyy <- rbf_kernel(Y, Y, sigma)
    Kxy <- rbf_kernel(X, Y, sigma)

    n <- nrow(X)
    m <- nrow(Y)

    # Unbiased estimator
    mmd2 <- (sum(Kxx) - sum(diag(Kxx))) / (n * (n - 1)) +
            (sum(Kyy) - sum(diag(Kyy))) / (m * (m - 1)) -
            2 * sum(Kxy) / (n * m)
    return(mmd2)
  }

  # Original statistic
  mmd2_orig <- compute_mmd2(X_train, X_test, sigma)

  # Permutation test
  mmd2_perm <- numeric(B)
  for (b in 1:B) {
    perm_idx <- sample(n + m, n, replace = FALSE)
    X_perm <- X_combined[perm_idx, , drop = FALSE]
    Y_perm <- X_combined[-perm_idx, , drop = FALSE]
    mmd2_perm[b] <- compute_mmd2(X_perm, Y_perm, sigma)
  }

  # P-value
  p_val <- (1 + sum(mmd2_perm >= mmd2_orig)) / (B + 1)
  return(list(p_value = p_val, mmd2 = mmd2_orig, sigma = sigma))
}

# Benchmark comparison wrapper
run_benchmark <- function(df_train, df_test, B = 500) {
  results <- list()

  # RFPerm
  t1 <- system.time({
    results$rfperm <- perm_oobtest(df_train, df_test, B = B)
  })
  results$rfperm_time <- t1[3]

  # CFPerm
  t2 <- system.time({
    results$cfperm <- cfperm(df_train, df_test, n_perm = B)
  })
  results$cfperm_time <- t2[3]

  # MMD (features only)
  X_train <- as.matrix(df_train[, -ncol(df_train)])
  X_test <- as.matrix(df_test[, -ncol(df_test)])
  t3 <- system.time({
    results$mmd <- mmd_test(X_train, X_test, B = B)$p_value
  })
  results$mmd_time <- t3[3]

  return(results)
}
\end{lstlisting}

%============================================================
\section{Real-Data Application}
%============================================================

\subsection{Dataset: California Housing Prices}

We demonstrate Sequential RFPerm on the California Housing dataset, simulating a temporal deployment scenario where housing market conditions shift over time.

\subsubsection{Setup}
\begin{itemize}
    \item \textbf{Reference period}: First 5,000 observations (representing stable market conditions)
    \item \textbf{Sequential batches}: Remaining data split into batches of 500, representing weekly/monthly data
    \item \textbf{Target}: Median house value
    \item \textbf{Features}: Location (lat/long), demographics, housing characteristics
\end{itemize}

\subsection{Application Code}

\begin{lstlisting}[caption={Real Data Application}]
# Load California Housing data
library(MASS)
data(Boston)  # Using Boston as proxy

# Prepare data
set.seed(42)
idx <- sample(nrow(Boston))
Boston <- Boston[idx, ]

# Reference period
df_ref <- Boston[1:250, ]
df_ref$Y <- df_ref$medv
df_ref <- df_ref[, !names(df_ref) %in% "medv"]

# Create sequential batches
n_batches <- 10
batch_size <- 25
batch_list <- lapply(1:n_batches, function(b) {
  start_idx <- 250 + (b-1) * batch_size + 1
  end_idx <- min(start_idx + batch_size - 1, nrow(Boston))
  batch <- Boston[start_idx:end_idx, ]
  batch$Y <- batch$medv
  batch <- batch[, !names(batch) %in% "medv"]
  return(batch)
})

# Run Sequential RFPerm
results <- seq_rfperm(df_ref, batch_list, alpha = 0.05)

# Display results
for(r in results) {
  cat(sprintf("Batch %d: p=%.4f, E=%.4f, Cumulative E=%.4f\n",
              r$batch, r$p_value, r$e_value, r$E_process))
}
\end{lstlisting}

\subsection{Results Interpretation}

The Sequential RFPerm procedure monitors incoming batches and accumulates evidence for drift:

\begin{table}[H]
\centering
\caption{Sequential Monitoring Results on Housing Data}
\begin{tabular}{ccccc}
\toprule
\textbf{Batch} & \textbf{P-value} & \textbf{E-value} & \textbf{Cum. E-process} & \textbf{Status} \\
\midrule
1 & 0.342 & 0.0 & 1.00 & No detection \\
2 & 0.218 & 0.0 & 1.00 & No detection \\
3 & 0.089 & 0.0 & 1.00 & No detection \\
4 & 0.041 & 2.94 & 2.94 & Accumulating \\
5 & 0.023 & 4.14 & 12.18 & Accumulating \\
6 & 0.018 & 4.71 & 57.37 & \textbf{Drift detected} \\
\bottomrule
\end{tabular}
\label{tab:realdata}
\end{table}

The procedure detects potential distribution shift at batch 6, warranting investigation into:
\begin{itemize}
    \item Feature distribution changes (geographic sampling differences)
    \item Target distribution shifts (price appreciation/depreciation)
    \item Model calibration degradation
\end{itemize}

%============================================================
\section{Discussion}
%============================================================

\subsection{Advantages of Sequential RFPerm}

\begin{enumerate}
    \item \textbf{Anytime validity}: Valid inference at any stopping time without multiple testing correction.

    \item \textbf{Computational efficiency}: Leverages OOB mechanism; no need to retrain models for each batch.

    \item \textbf{Model-agnostic}: Works with any RF implementation; easily extended to gradient boosting.

    \item \textbf{Interpretable}: P-values and e-values have clear statistical meaning.
\end{enumerate}

\subsection{Practical Recommendations}

\begin{itemize}
    \item \textbf{Calibrator choice}: Use $\kappa = 2$ for general settings; increase for detecting subtle drift.

    \item \textbf{Batch size}: Larger batches increase per-batch power but may delay detection.

    \item \textbf{Reference updates}: Consider periodic reference set updates for long-running systems.

    \item \textbf{Multiple metrics}: Combine prediction error with feature importance drift (CFPerm).
\end{itemize}

\subsection{Extensions}

Future work includes:
\begin{enumerate}
    \item Adaptive reference set updating
    \item Multi-stream monitoring
    \item Feature-level drift localization
    \item Integration with model retraining pipelines
\end{enumerate}

%============================================================
\section{Conclusion}
%============================================================

We presented Extended Sequential RFPerm, a powerful and theoretically grounded approach for online dataset shift detection. By combining the permutation testing framework with e-processes and betting martingales, our method achieves:

\begin{enumerate}
    \item Exact type I error control at any stopping time
    \item High power under various drift scenarios
    \item Practical applicability through efficient computation
\end{enumerate}

The method is particularly valuable for production ML systems requiring continuous monitoring with rigorous statistical guarantees.

%============================================================
\appendix
\section{Implementation Details}
%============================================================

\subsection{Full R Package Functions}

The complete implementation is available in the \texttt{RFPerm} R package at \url{https://github.com/[user]/RFPerm}.

Key functions:
\begin{itemize}
    \item \texttt{perm\_oobtest()}: Standard RFPerm test
    \item \texttt{seq\_rfperm()}: Sequential extension
    \item \texttt{cfperm()}: Causal Forest variant
\end{itemize}

\subsection{Computational Complexity}

\begin{itemize}
    \item Training: $O(n \cdot p \cdot T \cdot \log n)$ where $T$ = number of trees
    \item Per-batch testing: $O(m \cdot T + B \cdot (n + m))$ where $B$ = permutations
    \item Memory: $O(n \cdot T)$ for OOB tracking
\end{itemize}

%============================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem{mentch2016quantifying}
Mentch, L. and Hooker, G. (2016).
\newblock Quantifying uncertainty in random forests via confidence intervals and hypothesis tests.
\newblock {\em Journal of Machine Learning Research}, 17(1):841--881.

\bibitem{ramdas2023game}
Ramdas, A., Ruf, J., Larsson, M., and Koolen, W. (2023).
\newblock Game-theoretic statistics and safe anytime-valid inference.
\newblock {\em Statistical Science}, 38(4):576--601.

\bibitem{vovk2021values}
Vovk, V. and Wang, R. (2021).
\newblock E-values: Calibration, combination and applications.
\newblock {\em Annals of Statistics}, 49(3):1736--1754.

\bibitem{breiman2001random}
Breiman, L. (2001).
\newblock Random forests.
\newblock {\em Machine Learning}, 45(1):5--32.

\bibitem{shimodaira2000improving}
Shimodaira, H. (2000).
\newblock Improving predictive inference under covariate shift by weighting the log-likelihood function.
\newblock {\em Journal of Statistical Planning and Inference}, 90(2):227--244.

\bibitem{wager2018estimation}
Wager, S. and Athey, S. (2018).
\newblock Estimation and inference of heterogeneous treatment effects using random forests.
\newblock {\em Journal of the American Statistical Association}, 113(523):1228--1242.

\bibitem{gretton2012kernel}
Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch{\"o}lkopf, B., Smola, A., and others (2012).
\newblock A kernel two-sample test.
\newblock {\em Journal of Machine Learning Research}, 13(1):723--773.

\bibitem{lipton2018detecting}
Lipton, Z., Wang, Y.-X., and Smola, A. (2018).
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In {\em International Conference on Machine Learning}, pages 3122--3130.

\bibitem{josse2019measuring}
Josse, J., Prost, N., Scornet, E., and Varoquaux, G. (2019).
\newblock On the consistency of supervised learning with missing values.
\newblock {\em arXiv preprint arXiv:1902.06931}.

\end{thebibliography}

\end{document}
